# Training configuration for the FiLM-conditioned CNN with RBP/tRNA features

dataset_dir: data/processed/seq_cnn_v1_rbp_trna

batch_size: 64
num_workers: 8
max_cache_shards: 2  # limit shard residency to control RAM usage per worker
log_interval: 200
dataloader_seed: 0
prefetch_factor: 2
persistent_workers: true
log_level: INFO
epochs: 20
learning_rate: 1.5e-3
weight_decay: 1.0e-4

conv_channels: [64, 128, 256]
stem_channels: 32
film_dim: 32
hidden_dim: 256
dropout: 0.2

output_model_path: outputs/cnn_film_rbp_trna_best.pt

# Validation-specific dataloader overrides. These keep the training loader fast
# while trimming memory usage for the val split to avoid OOM. If the validation
# loader still runs out of memory, progressively lower `batch_size` (e.g. 16,
# 8) and, if needed, set `num_workers` to 0 so that batches are loaded on the
# main process.
split_loader_overrides:
  val:
    batch_size: 32
    num_workers: 1
    max_cache_shards: 1
    prefetch_factor: 1
    persistent_workers: false
